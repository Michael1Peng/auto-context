# Task ID: 11
# Title: Implement Smart Context Window Management
# Status: deferred
# Dependencies: 5, 6
# Priority: medium
# Description: Develop intelligent context window management to prioritize relevant files and optimize context size for AI systems.
# Details:
Create a ContextWindowManager class to intelligently manage context size and relevance:

```typescript
export interface ContextWindowOptions {
  maxTokens?: number;
  maxFiles?: number;
  prioritizeActiveFile?: boolean;
  prioritizePinnedTabs?: boolean;
  tokensPerFile?: number;
}

export class ContextWindowManager {
  private options: ContextWindowOptions;
  
  constructor(options?: Partial<ContextWindowOptions>) {
    this.options = {
      maxTokens: options?.maxTokens || 8000, // Default token limit
      maxFiles: options?.maxFiles || 20,
      prioritizeActiveFile: options?.prioritizeActiveFile !== undefined ? 
        options.prioritizeActiveFile : true,
      prioritizePinnedTabs: options?.prioritizePinnedTabs !== undefined ? 
        options.prioritizePinnedTabs : true,
      tokensPerFile: options?.tokensPerFile || 400 // Approximate tokens per file
    };
  }
  
  public optimizeContext(files: FileData[], activeFile?: string, pinnedFiles?: string[]): FileData[] {
    // Make a copy to avoid modifying the original
    let optimizedFiles = [...files];
    
    // Sort files by priority
    optimizedFiles.sort((a, b) => {
      // Active file gets highest priority
      if (this.options.prioritizeActiveFile && activeFile) {
        if (a.filePath === activeFile) return -1;
        if (b.filePath === activeFile) return 1;
      }
      
      // Pinned tabs get next priority
      if (this.options.prioritizePinnedTabs && pinnedFiles?.length) {
        const aIsPinned = pinnedFiles.includes(a.filePath);
        const bIsPinned = pinnedFiles.includes(b.filePath);
        if (aIsPinned && !bIsPinned) return -1;
        if (!aIsPinned && bIsPinned) return 1;
      }
      
      // Default to alphabetical order
      return a.filePath.localeCompare(b.filePath);
    });
    
    // Limit by max files
    if (this.options.maxFiles && optimizedFiles.length > this.options.maxFiles) {
      optimizedFiles = optimizedFiles.slice(0, this.options.maxFiles);
    }
    
    // Estimate token count and trim if needed
    let totalTokens = 0;
    const result: FileData[] = [];
    
    for (const file of optimizedFiles) {
      // Rough estimate of tokens (characters / 4)
      const fileTokens = Math.ceil(file.content.length / 4);
      
      if (totalTokens + fileTokens <= this.options.maxTokens!) {
        result.push(file);
        totalTokens += fileTokens;
      } else if (result.length === 0) {
        // Always include at least one file, but truncate content
        const truncatedContent = file.content.substring(
          0, 
          this.options.maxTokens! * 4
        );
        result.push({
          filePath: file.filePath,
          content: truncatedContent + '\n// Content truncated due to size limits'
        });
        break;
      } else {
        // We've reached the token limit
        break;
      }
    }
    
    return result;
  }
}
```

Implement token counting based on OpenAI's tokenization approach (approximately 4 characters per token). Add configuration options for maximum context size. Prioritize files based on relevance (active file, pinned tabs, recently edited). Support truncating large files to fit within context window. Add option to include file summaries instead of full content for large files.

# Test Strategy:
1. Test file prioritization with various input scenarios
2. Verify token counting and limiting works correctly
3. Test with large files to ensure proper truncation
4. Verify active file and pinned tabs are correctly prioritized
5. Benchmark performance with large file sets
