## 背景: 这个插件是为了解决什么问题？

### 如何获取上下文？

LLM 很强大，能生成非常长且完善的代码，并通过 agentic 的能力来执行命令和调用工具来调试。但是面对大公司内部的大型项目，LLM 输出代码有一个核心痛点，难以准确地获取仓库内部的知识库和编码习惯。

目前主流 IDE cursor 和 vscode 插件 cline 都支持配置 rules 文件来固定设置 LLM 的 system prompt，从而达到喂给 LLM 知识库的效果。而一个大型项目可能有10个研发在开发，每个需求具体需要的知识库和需求上下文都不一致。如果把每个需求的上下文都完整的塞到 rules 文件里，会有两个问题：1. 上下文太多很容易导致请求爆上下文；2. 过多的 context 信息导致 AI 编码指令准确性下降。

所以解决大型仓库内 LLM 编码上下文问题比较好的解法应该是动态修改上下文。修改上下文有两种方式：1. AI 动态主动获取上下文；2. 配置化方式静态更新。

### 如何动态获取上下文？

个人理解，“AI 动态主动获取上下文”经历过两个阶段：

1. 基于 rag 流程来获取：把长文本内容 embedding 后再通过 similarity search 和 rerank 来获取上下文，优点是基本能处理无限大上下文，缺点是文件内容 embedding 过后丢失了内容直接的关联性，导致召回内容不完整，同一个组件的 api 和 demo 可能没有被一起召回。

2. 第二阶段：AI 通过 agentic 的能力主动获取上下文，代表方案是 cursor 的 composer 功能，通过 search、read_file 和 mcp tool 来获取上下文，智能程度显著提升，但是同样解决不了准确性和上下文长度“偷懒”的问题。

cursor 和 trae 这种基于 agentic composer 主动获取上下文的方案，由于缺少高质量判断上下文信息充足的方案，同时一次 agentic 请求的上下文和 call tool 次数有限，导致获取的信息可能有缺失。

### 如何配置化更新上下文？

从长远来看，未来最好的获取上下文的方式会是 AI 主动获取的方式，这样用户的使用成本基本最小。但是短期为了提升准确率和稳定性，配置化获取上下文的方式也是一个很重要的过渡能力。

配置化获取上下文的步骤：1. 明确需求，分析需要找什么内容；2. 通过对入口文件的依赖分析找到相关的文件和文件夹；3. 生成配置；4. 通过配置获取固定的文件夹内容到 rules 文件，这样 chat 和 agent 请求的时候就能获取到上下文。

研发开发一个需求的过程，找相关内容也需要重复第一步和第二步，可以通过 vscode 插件介入到研发流程的第一步，根据用户自然的交互场景，来自动化第二步、第三步和第四步。
